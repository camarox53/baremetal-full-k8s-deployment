---
# Install the latest EPEL Repo 
- name: Install latest EPEL
  package:
    name: http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
    state: present

# Enable GlusterFS Repo (Latest stable LTS is 312)
- name: Enable GlusterFS repo
  package:
    name: centos-release-gluster312
    state: present

# Install Packages that GlusterFS needs
- name: Install required packages for GlusterFS
  package:
    name: "{{ item }}"
    state: present
  with_items:
    - glusterfs
    - glusterfs-cli
    - glusterfs-libs
    - glusterfs-server
    - samba

# Down with iptables
- name: Flush iptables rules
  command: iptables -F 
  command: iptables-save 

# Start the glusterd service
- name: Start gluster service
  service:
    name: glusterd
    state: restarted
    enabled: yes

# Delete custom lock files
- name: DELETE SEMAPHORS TO REBUILD VOLUME GROUPS
  file:
    dest: "{{ item }}"
    state: absent
  with_items:
    - "/etc/.pvcreated"
    - "/etc/.vgcreated"
    - "/etc/.brickscreated"
    - "/etc/.xfssetup"

# Run these to remove the volume group (tends to break things if you automate this) 
- name: thing
  command: echo "y" | gluster volume stop glustervol1
  command: echo "y" | gluster volume delete glustervol1
#  when: "'cnode001' in inventory_hostname"

# Stop the glusterd service
- name: Stop gluster service
  service:
    name: glusterd
    state: stopped
    enabled: yes

- name: Unmount volume groups 
  command: umount -f /dev/mapper/vg_gluster-brick1
  command: umount -f /dev/vg_gluster/brick1
  command: umount -f /srv/brick1
  ignore_errors: True

- name: USED ONLY FOR DELETEING VOLUME GROUPS
  shell: >
    yes | lvremove -f /dev/vg_gluster/brick1;
  ignore_errors: yes

- name: Overwrite partition table on data drive
  command: dd if=/dev/zero of=/dev/vdb bs=512 count=1

# Creates the physical Volume on Data qcow
- name: Create a new Physical Volume
  shell: >
    yes | pvcreate -ff /dev/vdb &&
    touch /etc/.pvcreated
  args:
    creates: /etc/.pvcreated

# Creates a Volume group called vg_gluster on Data qcow 
- name: Create a Volume Group
  shell: >
    vgcreate vg_gluster /dev/vdb &&
    touch /etc/.vgcreated
  args:
    creates: /etc/.vgcreated

# Creates an XFS Brick known as Brick1
- name: Create the brick1 Logical Volume
  shell: >
    lvcreate -L 30G -n brick1 vg_gluster &&
    touch /etc/.brickscreated
  args:
    creates: /etc/.brickscreated

- name: holla 
  command: dd if=/dev/zero of=/dev/vg_gluster/brick1 bs=512 count=1
- name: Unmount volume groups 
  command: umount -f /dev/mapper/vg_gluster-brick1
  command: umount -f /dev/vg_gluster/brick1
  command: umount -f /srv/brick1
  ignore_errors: True

# Installs the XFS file system to the gluster brick
- name: Setup XFS file systems
  shell: >
    mkfs.xfs -f /dev/vg_gluster/brick1 &&
    touch /etc/.xfssetup
  args:
    creates: /etc/.xfssetup

# Creates a directory to mount the Volume group to 
- name: Create mount points
  file:
    dest: "/srv/brick{{ item }}"
    state: directory
  with_items:
    - 1

# Fstab Entries: 
# /dev/vg_gluster/brick1  /bricks/brick1    xfs     defaults    0 0

# Mount the Volume groups to the mount points we created earlier
- name: Mount XFS bricks
  mount:
    name: "/srv/brick{{ item }}"
    src: "/dev/vg_gluster/brick{{ item }}"
    fstype: xfs
    opts: rw
    state: mounted
  with_items:
    - 1

# Start the glusterd service
- name: Start gluster service
  service:
    name: glusterd
    state: started
    enabled: yes

# Creates a high availibility brick1
- name: Create brick directory
  file:
    dest: "/srv/brick1/brick1HA"
    state: directory

# Get list of all cnodes in k8s-all-nodes group in inventory file
- name: Initialize fact for list of k8s-all-nodess
  set_fact:
#    host_brick_string: "{{ hostvars[inventory_hostname]['inventory_hostname'] }}.prete.lcsee.wvu.edu:/srv/brick1/brick1HA "
    host_brick_string: ""
    number_hosts: "{{ ( groups['k8s-all-nodes'] | length ) }}"

# Generate the GlusterFS Connect strings
- name: Loop through and create "host:/srv/" string given list of cnodes
  set_fact:
    host_brick_string: "{{ host_brick_string }}{{ hostvars[item]['inventory_hostname'] }}:/srv/brick1/brick1HA "
#    host_brick_string: "{{ hostvars[item]['inventory_hostname'] }}:/srv/brick1/brick1HA"
  with_items: "{{ groups['k8s-all-nodes'] }}"

#- name: Debug list of gluster volume hosts
#  debug: "msg={{host_brick_string}}"
#- name: Debug number of replicas
#  debug: "msg={{number_hosts}}"

# Get status of the gluster peers
- name: Get peer status.
  shell: >
    gluster peer status
  register: peer_status
  when: "'cnode001' in inventory_hostname"

# You can delete a peer with:
# [root@kube-master ~]# gluster peer detach 192.168.122.17

- name: Probe detach all nodes
  shell: >
    gluster peer detach {{ hostvars[item]['inventory_hostname'] }}
#  when: "hostvars[item]['inventory_hostname'] not in peer_status.stdout"
  with_items: "{{ groups['k8s-all-nodes'] }}"
  ignore_errors: True

# Add the peers that are not showing up in a status 
- name: Probe each peer not in the status.
  shell: >
    gluster peer probe {{ hostvars[item]['inventory_hostname'] }}
  when: "hostvars[item]['inventory_hostname'] not in peer_status.stdout"
  with_items: "{{ groups['k8s-all-nodes'] }}"
  ignore_errors: True
  when: "'cnode001' in inventory_hostname"
  
# Create a list of all the GlusterFS Volumes
- name: Get list of gluster volumes
  shell: >
    gluster volume list
  register: volume_list
  when: "'cnode001' in inventory_hostname"

# Create the GlusterFS Volumes
- name: Create glusterFS volume
  shell: >
    gluster volume create glustervol1 replica {{ number_hosts }} transport tcp {{ host_brick_string }}
  when: '"glustervol" ~ 1 not in volume_list.stdout'
  when: "'cnode001' in inventory_hostname"
  ignore_errors: True

# Check the status of the glustervolume
- name: Get status of volume
  shell: >
    gluster volume status glustervol1
  register: volume_status
  ignore_errors: yes
  when: "'cnode001' in inventory_hostname"

# Start the gluster Volume
- name: Start gluster volume
  shell: >
    gluster volume start glustervol1 force
  when: "volume_status.rc != 0"
  when: "'cnode001' in inventory_hostname"

